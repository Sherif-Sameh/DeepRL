{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x754f3e310830>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h0, c0):\n",
    "        out, (hidden, cell) = self.rnn(x, (h0, c0))\n",
    "        out = self.fc(out)  # Apply linear layer to all time steps\n",
    "        return out, (hidden, cell)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1       # Each input is a single feature\n",
    "hidden_size = 16     # Size of hidden state\n",
    "output_size = 1      # Predicting a single value\n",
    "seq_len = 10         # Length of each sequence\n",
    "batch_size = 4       # Number of sequences in a batch\n",
    "num_epochs = 200      # Number of training epochs\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "# Example: input sequences are sinusoids, and targets are shifted versions\n",
    "x_train = torch.sin(torch.linspace(0, 2 * 3.1416, seq_len * batch_size).view(batch_size, seq_len, 1))\n",
    "y_train = torch.cos(torch.linspace(0, 2 * 3.1416, seq_len * batch_size).view(batch_size, seq_len, 1))\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape torch.Size([4, 10, 1]), hidden state shape: torch.Size([1, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "in_data = torch.randn((batch_size, seq_len, input_size))\n",
    "h0 = torch.zeros((1, batch_size, hidden_size))\n",
    "c0 = torch.zeros((1, batch_size, hidden_size))\n",
    "out, (hidden, cell) = model(in_data, h0, c0)\n",
    "\n",
    "print(f'Output shape {out.shape}, hidden state shape: {hidden.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.5224\n",
      "Epoch [2/200], Loss: 0.5152\n",
      "Epoch [3/200], Loss: 0.5112\n",
      "Epoch [4/200], Loss: 0.5067\n",
      "Epoch [5/200], Loss: 0.5016\n",
      "Epoch [6/200], Loss: 0.4964\n",
      "Epoch [7/200], Loss: 0.4909\n",
      "Epoch [8/200], Loss: 0.4840\n",
      "Epoch [9/200], Loss: 0.4753\n",
      "Epoch [10/200], Loss: 0.4651\n",
      "Epoch [11/200], Loss: 0.4541\n",
      "Epoch [12/200], Loss: 0.4430\n",
      "Epoch [13/200], Loss: 0.4332\n",
      "Epoch [14/200], Loss: 0.4256\n",
      "Epoch [15/200], Loss: 0.4164\n",
      "Epoch [16/200], Loss: 0.3996\n",
      "Epoch [17/200], Loss: 0.3786\n",
      "Epoch [18/200], Loss: 0.3664\n",
      "Epoch [19/200], Loss: 0.3481\n",
      "Epoch [20/200], Loss: 0.3366\n",
      "Epoch [21/200], Loss: 0.3370\n",
      "Epoch [22/200], Loss: 0.3387\n",
      "Epoch [23/200], Loss: 0.3391\n",
      "Epoch [24/200], Loss: 0.3386\n",
      "Epoch [25/200], Loss: 0.3376\n",
      "Epoch [26/200], Loss: 0.3356\n",
      "Epoch [27/200], Loss: 0.3309\n",
      "Epoch [28/200], Loss: 0.3213\n",
      "Epoch [29/200], Loss: 0.3110\n",
      "Epoch [30/200], Loss: 0.3186\n",
      "Epoch [31/200], Loss: 0.3047\n",
      "Epoch [32/200], Loss: 0.3050\n",
      "Epoch [33/200], Loss: 0.3073\n",
      "Epoch [34/200], Loss: 0.2990\n",
      "Epoch [35/200], Loss: 0.2922\n",
      "Epoch [36/200], Loss: 0.2921\n",
      "Epoch [37/200], Loss: 0.2844\n",
      "Epoch [38/200], Loss: 0.2763\n",
      "Epoch [39/200], Loss: 0.2733\n",
      "Epoch [40/200], Loss: 0.2693\n",
      "Epoch [41/200], Loss: 0.2622\n",
      "Epoch [42/200], Loss: 0.2558\n",
      "Epoch [43/200], Loss: 0.2519\n",
      "Epoch [44/200], Loss: 0.2476\n",
      "Epoch [45/200], Loss: 0.2404\n",
      "Epoch [46/200], Loss: 0.2320\n",
      "Epoch [47/200], Loss: 0.2244\n",
      "Epoch [48/200], Loss: 0.2174\n",
      "Epoch [49/200], Loss: 0.2105\n",
      "Epoch [50/200], Loss: 0.2051\n",
      "Epoch [51/200], Loss: 0.2020\n",
      "Epoch [52/200], Loss: 0.2011\n",
      "Epoch [53/200], Loss: 0.2010\n",
      "Epoch [54/200], Loss: 0.1988\n",
      "Epoch [55/200], Loss: 0.1940\n",
      "Epoch [56/200], Loss: 0.1878\n",
      "Epoch [57/200], Loss: 0.1826\n",
      "Epoch [58/200], Loss: 0.1787\n",
      "Epoch [59/200], Loss: 0.1757\n",
      "Epoch [60/200], Loss: 0.1733\n",
      "Epoch [61/200], Loss: 0.1710\n",
      "Epoch [62/200], Loss: 0.1688\n",
      "Epoch [63/200], Loss: 0.1658\n",
      "Epoch [64/200], Loss: 0.1619\n",
      "Epoch [65/200], Loss: 0.1576\n",
      "Epoch [66/200], Loss: 0.1534\n",
      "Epoch [67/200], Loss: 0.1502\n",
      "Epoch [68/200], Loss: 0.1483\n",
      "Epoch [69/200], Loss: 0.1468\n",
      "Epoch [70/200], Loss: 0.1456\n",
      "Epoch [71/200], Loss: 0.1434\n",
      "Epoch [72/200], Loss: 0.1404\n",
      "Epoch [73/200], Loss: 0.1370\n",
      "Epoch [74/200], Loss: 0.1340\n",
      "Epoch [75/200], Loss: 0.1320\n",
      "Epoch [76/200], Loss: 0.1307\n",
      "Epoch [77/200], Loss: 0.1295\n",
      "Epoch [78/200], Loss: 0.1276\n",
      "Epoch [79/200], Loss: 0.1255\n",
      "Epoch [80/200], Loss: 0.1233\n",
      "Epoch [81/200], Loss: 0.1216\n",
      "Epoch [82/200], Loss: 0.1201\n",
      "Epoch [83/200], Loss: 0.1185\n",
      "Epoch [84/200], Loss: 0.1169\n",
      "Epoch [85/200], Loss: 0.1151\n",
      "Epoch [86/200], Loss: 0.1135\n",
      "Epoch [87/200], Loss: 0.1129\n",
      "Epoch [88/200], Loss: 0.1138\n",
      "Epoch [89/200], Loss: 0.1110\n",
      "Epoch [90/200], Loss: 0.1086\n",
      "Epoch [91/200], Loss: 0.1087\n",
      "Epoch [92/200], Loss: 0.1067\n",
      "Epoch [93/200], Loss: 0.1046\n",
      "Epoch [94/200], Loss: 0.1047\n",
      "Epoch [95/200], Loss: 0.1027\n",
      "Epoch [96/200], Loss: 0.1017\n",
      "Epoch [97/200], Loss: 0.1014\n",
      "Epoch [98/200], Loss: 0.0990\n",
      "Epoch [99/200], Loss: 0.0990\n",
      "Epoch [100/200], Loss: 0.0976\n",
      "Epoch [101/200], Loss: 0.0965\n",
      "Epoch [102/200], Loss: 0.0961\n",
      "Epoch [103/200], Loss: 0.0948\n",
      "Epoch [104/200], Loss: 0.0939\n",
      "Epoch [105/200], Loss: 0.0934\n",
      "Epoch [106/200], Loss: 0.0921\n",
      "Epoch [107/200], Loss: 0.0916\n",
      "Epoch [108/200], Loss: 0.0909\n",
      "Epoch [109/200], Loss: 0.0898\n",
      "Epoch [110/200], Loss: 0.0894\n",
      "Epoch [111/200], Loss: 0.0884\n",
      "Epoch [112/200], Loss: 0.0877\n",
      "Epoch [113/200], Loss: 0.0872\n",
      "Epoch [114/200], Loss: 0.0864\n",
      "Epoch [115/200], Loss: 0.0858\n",
      "Epoch [116/200], Loss: 0.0852\n",
      "Epoch [117/200], Loss: 0.0846\n",
      "Epoch [118/200], Loss: 0.0839\n",
      "Epoch [119/200], Loss: 0.0834\n",
      "Epoch [120/200], Loss: 0.0829\n",
      "Epoch [121/200], Loss: 0.0821\n",
      "Epoch [122/200], Loss: 0.0817\n",
      "Epoch [123/200], Loss: 0.0812\n",
      "Epoch [124/200], Loss: 0.0805\n",
      "Epoch [125/200], Loss: 0.0800\n",
      "Epoch [126/200], Loss: 0.0795\n",
      "Epoch [127/200], Loss: 0.0790\n",
      "Epoch [128/200], Loss: 0.0785\n",
      "Epoch [129/200], Loss: 0.0780\n",
      "Epoch [130/200], Loss: 0.0775\n",
      "Epoch [131/200], Loss: 0.0771\n",
      "Epoch [132/200], Loss: 0.0767\n",
      "Epoch [133/200], Loss: 0.0768\n",
      "Epoch [134/200], Loss: 0.0779\n",
      "Epoch [135/200], Loss: 0.0823\n",
      "Epoch [136/200], Loss: 0.0762\n",
      "Epoch [137/200], Loss: 0.0847\n",
      "Epoch [138/200], Loss: 0.0775\n",
      "Epoch [139/200], Loss: 0.0817\n",
      "Epoch [140/200], Loss: 0.0774\n",
      "Epoch [141/200], Loss: 0.0741\n",
      "Epoch [142/200], Loss: 0.0761\n",
      "Epoch [143/200], Loss: 0.0775\n",
      "Epoch [144/200], Loss: 0.0753\n",
      "Epoch [145/200], Loss: 0.0731\n",
      "Epoch [146/200], Loss: 0.0733\n",
      "Epoch [147/200], Loss: 0.0744\n",
      "Epoch [148/200], Loss: 0.0743\n",
      "Epoch [149/200], Loss: 0.0730\n",
      "Epoch [150/200], Loss: 0.0720\n",
      "Epoch [151/200], Loss: 0.0719\n",
      "Epoch [152/200], Loss: 0.0724\n",
      "Epoch [153/200], Loss: 0.0722\n",
      "Epoch [154/200], Loss: 0.0712\n",
      "Epoch [155/200], Loss: 0.0704\n",
      "Epoch [156/200], Loss: 0.0702\n",
      "Epoch [157/200], Loss: 0.0703\n",
      "Epoch [158/200], Loss: 0.0700\n",
      "Epoch [159/200], Loss: 0.0694\n",
      "Epoch [160/200], Loss: 0.0688\n",
      "Epoch [161/200], Loss: 0.0685\n",
      "Epoch [162/200], Loss: 0.0685\n",
      "Epoch [163/200], Loss: 0.0682\n",
      "Epoch [164/200], Loss: 0.0677\n",
      "Epoch [165/200], Loss: 0.0673\n",
      "Epoch [166/200], Loss: 0.0671\n",
      "Epoch [167/200], Loss: 0.0669\n",
      "Epoch [168/200], Loss: 0.0666\n",
      "Epoch [169/200], Loss: 0.0662\n",
      "Epoch [170/200], Loss: 0.0659\n",
      "Epoch [171/200], Loss: 0.0657\n",
      "Epoch [172/200], Loss: 0.0655\n",
      "Epoch [173/200], Loss: 0.0652\n",
      "Epoch [174/200], Loss: 0.0649\n",
      "Epoch [175/200], Loss: 0.0647\n",
      "Epoch [176/200], Loss: 0.0646\n",
      "Epoch [177/200], Loss: 0.0645\n",
      "Epoch [178/200], Loss: 0.0647\n",
      "Epoch [179/200], Loss: 0.0654\n",
      "Epoch [180/200], Loss: 0.0667\n",
      "Epoch [181/200], Loss: 0.0653\n",
      "Epoch [182/200], Loss: 0.0636\n",
      "Epoch [183/200], Loss: 0.0635\n",
      "Epoch [184/200], Loss: 0.0634\n",
      "Epoch [185/200], Loss: 0.0636\n",
      "Epoch [186/200], Loss: 0.0629\n",
      "Epoch [187/200], Loss: 0.0621\n",
      "Epoch [188/200], Loss: 0.0626\n",
      "Epoch [189/200], Loss: 0.0622\n",
      "Epoch [190/200], Loss: 0.0618\n",
      "Epoch [191/200], Loss: 0.0615\n",
      "Epoch [192/200], Loss: 0.0614\n",
      "Epoch [193/200], Loss: 0.0613\n",
      "Epoch [194/200], Loss: 0.0607\n",
      "Epoch [195/200], Loss: 0.0608\n",
      "Epoch [196/200], Loss: 0.0606\n",
      "Epoch [197/200], Loss: 0.0602\n",
      "Epoch [198/200], Loss: 0.0601\n",
      "Epoch [199/200], Loss: 0.0600\n",
      "Epoch [200/200], Loss: 0.0598\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize the hidden state\n",
    "    hidden = torch.zeros(1, batch_size, hidden_size)  # Shape: (num_layers, batch_size, hidden_size)\n",
    "    cell = torch.zeros(1, batch_size, hidden_size)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, (hidden, cell) = model(x_train, hidden, cell)\n",
    "    loss = criterion(outputs, y_train)  # Compute loss for all time steps\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample output (first sequence):\n",
      "Predicted: [-0.02943501  0.5567978   0.9707504   0.92432016  0.7900776   0.663672\n",
      "  0.5660973   0.45813662  0.31116766  0.11457111]\n",
      "Target:    [1.         0.9870502  0.9485362  0.8854555  0.7994419  0.69272304\n",
      " 0.5680629  0.42869017 0.2782146  0.12053337]\n"
     ]
    }
   ],
   "source": [
    "# Test the model (inference)\n",
    "with torch.no_grad():\n",
    "    test_hidden = torch.zeros(1, batch_size, hidden_size)\n",
    "    test_cell = torch.zeros(1, batch_size, hidden_size)\n",
    "    test_outputs, _ = model(x_train, test_hidden, test_cell)\n",
    "    print(\"\\nSample output (first sequence):\")\n",
    "    print(\"Predicted:\", test_outputs[0].squeeze().numpy())\n",
    "    print(\"Target:   \", y_train[0].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original IDs: [[1 1 1 2 2 2 2 3 3 4 4 4 5]\n",
      " [1 1 1 2 2 2 2 3 3 4 4 4 5]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 2 2 2 2 3 3 4 4 4 5]]\n",
      "Resulting Boolean Array: [[ True  True  True False False  True  True False False False False  True\n",
      "  False]\n",
      " [ True  True  True False False  True  True False False False False  True\n",
      "  False]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True]\n",
      " [ True  True  True False False  True  True False False False False  True\n",
      "  False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example input array of IDs\n",
    "ids = np.array([1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 5])\n",
    "ids = np.repeat(ids[None, :], repeats=4, axis=0)\n",
    "ids[2, :] = 1\n",
    "\n",
    "# Length of False values to add after each change\n",
    "false_length = 2\n",
    "result = np.full(ids.shape, fill_value=True)\n",
    "\n",
    "for i in range(ids.shape[0]):\n",
    "    # Step 1: Find where changes occur\n",
    "    change_indices = np.nonzero(np.diff(ids[i]) != 0)[0] + 1\n",
    "\n",
    "    # Step 2: Create an output array\n",
    "    # Initialize the result as an empty array\n",
    "    if change_indices.size > 0:\n",
    "        indices = np.array([np.arange(idx, idx+false_length) for idx in change_indices])\n",
    "        indices = np.minimum(indices, ids[i].shape[0]-1)\n",
    "        result[i, indices] = False\n",
    "\n",
    "print(\"Original IDs:\", ids)\n",
    "# print(\"Change indices:\", change_indices)\n",
    "print(\"Resulting Boolean Array:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
