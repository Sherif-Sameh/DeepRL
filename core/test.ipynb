{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7163935008d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h0, c0):\n",
    "        out, (hidden, cell) = self.rnn(x, (h0, c0))\n",
    "        out = self.fc(out)  # Apply linear layer to all time steps\n",
    "        return out, (hidden, cell)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1       # Each input is a single feature\n",
    "hidden_size = 16     # Size of hidden state\n",
    "output_size = 1      # Predicting a single value\n",
    "seq_len = 10         # Length of each sequence\n",
    "batch_size = 4       # Number of sequences in a batch\n",
    "num_epochs = 200      # Number of training epochs\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "# Example: input sequences are sinusoids, and targets are shifted versions\n",
    "x_train = torch.sin(torch.linspace(0, 2 * 3.1416, seq_len * batch_size).view(batch_size, seq_len, 1))\n",
    "y_train = torch.cos(torch.linspace(0, 2 * 3.1416, seq_len * batch_size).view(batch_size, seq_len, 1))\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape torch.Size([10, 1]), hidden state shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "in_data = torch.randn((batch_size, seq_len, input_size))\n",
    "h0 = torch.zeros((1, batch_size, hidden_size))\n",
    "c0 = torch.zeros((1, batch_size, hidden_size))\n",
    "out, (hidden, cell) = model(in_data, h0, c0)\n",
    "\n",
    "print(f'Output shape {out.shape}, hidden state shape: {hidden.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.4682\n",
      "Epoch [2/200], Loss: 0.4590\n",
      "Epoch [3/200], Loss: 0.4501\n",
      "Epoch [4/200], Loss: 0.4413\n",
      "Epoch [5/200], Loss: 0.4321\n",
      "Epoch [6/200], Loss: 0.4219\n",
      "Epoch [7/200], Loss: 0.4102\n",
      "Epoch [8/200], Loss: 0.3968\n",
      "Epoch [9/200], Loss: 0.3827\n",
      "Epoch [10/200], Loss: 0.3705\n",
      "Epoch [11/200], Loss: 0.3622\n",
      "Epoch [12/200], Loss: 0.3584\n",
      "Epoch [13/200], Loss: 0.3576\n",
      "Epoch [14/200], Loss: 0.3567\n",
      "Epoch [15/200], Loss: 0.3538\n",
      "Epoch [16/200], Loss: 0.3493\n",
      "Epoch [17/200], Loss: 0.3443\n",
      "Epoch [18/200], Loss: 0.3399\n",
      "Epoch [19/200], Loss: 0.3365\n",
      "Epoch [20/200], Loss: 0.3343\n",
      "Epoch [21/200], Loss: 0.3331\n",
      "Epoch [22/200], Loss: 0.3320\n",
      "Epoch [23/200], Loss: 0.3304\n",
      "Epoch [24/200], Loss: 0.3280\n",
      "Epoch [25/200], Loss: 0.3250\n",
      "Epoch [26/200], Loss: 0.3221\n",
      "Epoch [27/200], Loss: 0.3196\n",
      "Epoch [28/200], Loss: 0.3179\n",
      "Epoch [29/200], Loss: 0.3168\n",
      "Epoch [30/200], Loss: 0.3160\n",
      "Epoch [31/200], Loss: 0.3151\n",
      "Epoch [32/200], Loss: 0.3139\n",
      "Epoch [33/200], Loss: 0.3125\n",
      "Epoch [34/200], Loss: 0.3109\n",
      "Epoch [35/200], Loss: 0.3095\n",
      "Epoch [36/200], Loss: 0.3082\n",
      "Epoch [37/200], Loss: 0.3070\n",
      "Epoch [38/200], Loss: 0.3057\n",
      "Epoch [39/200], Loss: 0.3042\n",
      "Epoch [40/200], Loss: 0.3025\n",
      "Epoch [41/200], Loss: 0.3008\n",
      "Epoch [42/200], Loss: 0.2989\n",
      "Epoch [43/200], Loss: 0.2967\n",
      "Epoch [44/200], Loss: 0.2940\n",
      "Epoch [45/200], Loss: 0.2906\n",
      "Epoch [46/200], Loss: 0.2868\n",
      "Epoch [47/200], Loss: 0.2829\n",
      "Epoch [48/200], Loss: 0.2792\n",
      "Epoch [49/200], Loss: 0.2757\n",
      "Epoch [50/200], Loss: 0.2721\n",
      "Epoch [51/200], Loss: 0.2680\n",
      "Epoch [52/200], Loss: 0.2636\n",
      "Epoch [53/200], Loss: 0.2588\n",
      "Epoch [54/200], Loss: 0.2540\n",
      "Epoch [55/200], Loss: 0.2493\n",
      "Epoch [56/200], Loss: 0.2447\n",
      "Epoch [57/200], Loss: 0.2400\n",
      "Epoch [58/200], Loss: 0.2356\n",
      "Epoch [59/200], Loss: 0.2310\n",
      "Epoch [60/200], Loss: 0.2262\n",
      "Epoch [61/200], Loss: 0.2217\n",
      "Epoch [62/200], Loss: 0.2163\n",
      "Epoch [63/200], Loss: 0.2108\n",
      "Epoch [64/200], Loss: 0.2051\n",
      "Epoch [65/200], Loss: 0.1999\n",
      "Epoch [66/200], Loss: 0.1949\n",
      "Epoch [67/200], Loss: 0.1906\n",
      "Epoch [68/200], Loss: 0.1863\n",
      "Epoch [69/200], Loss: 0.1812\n",
      "Epoch [70/200], Loss: 0.1749\n",
      "Epoch [71/200], Loss: 0.1701\n",
      "Epoch [72/200], Loss: 0.1659\n",
      "Epoch [73/200], Loss: 0.1636\n",
      "Epoch [74/200], Loss: 0.1582\n",
      "Epoch [75/200], Loss: 0.1536\n",
      "Epoch [76/200], Loss: 0.1489\n",
      "Epoch [77/200], Loss: 0.1444\n",
      "Epoch [78/200], Loss: 0.1417\n",
      "Epoch [79/200], Loss: 0.1427\n",
      "Epoch [80/200], Loss: 0.1517\n",
      "Epoch [81/200], Loss: 0.1343\n",
      "Epoch [82/200], Loss: 0.1613\n",
      "Epoch [83/200], Loss: 0.1384\n",
      "Epoch [84/200], Loss: 0.1587\n",
      "Epoch [85/200], Loss: 0.1338\n",
      "Epoch [86/200], Loss: 0.1292\n",
      "Epoch [87/200], Loss: 0.1429\n",
      "Epoch [88/200], Loss: 0.1225\n",
      "Epoch [89/200], Loss: 0.1260\n",
      "Epoch [90/200], Loss: 0.1326\n",
      "Epoch [91/200], Loss: 0.1236\n",
      "Epoch [92/200], Loss: 0.1166\n",
      "Epoch [93/200], Loss: 0.1222\n",
      "Epoch [94/200], Loss: 0.1213\n",
      "Epoch [95/200], Loss: 0.1133\n",
      "Epoch [96/200], Loss: 0.1131\n",
      "Epoch [97/200], Loss: 0.1160\n",
      "Epoch [98/200], Loss: 0.1125\n",
      "Epoch [99/200], Loss: 0.1080\n",
      "Epoch [100/200], Loss: 0.1088\n",
      "Epoch [101/200], Loss: 0.1092\n",
      "Epoch [102/200], Loss: 0.1053\n",
      "Epoch [103/200], Loss: 0.1030\n",
      "Epoch [104/200], Loss: 0.1038\n",
      "Epoch [105/200], Loss: 0.1030\n",
      "Epoch [106/200], Loss: 0.1002\n",
      "Epoch [107/200], Loss: 0.0994\n",
      "Epoch [108/200], Loss: 0.0998\n",
      "Epoch [109/200], Loss: 0.0983\n",
      "Epoch [110/200], Loss: 0.0961\n",
      "Epoch [111/200], Loss: 0.0978\n",
      "Epoch [112/200], Loss: 0.0973\n",
      "Epoch [113/200], Loss: 0.0972\n",
      "Epoch [114/200], Loss: 0.0962\n",
      "Epoch [115/200], Loss: 0.0957\n",
      "Epoch [116/200], Loss: 0.0944\n",
      "Epoch [117/200], Loss: 0.0933\n",
      "Epoch [118/200], Loss: 0.0939\n",
      "Epoch [119/200], Loss: 0.0940\n",
      "Epoch [120/200], Loss: 0.0926\n",
      "Epoch [121/200], Loss: 0.0914\n",
      "Epoch [122/200], Loss: 0.0907\n",
      "Epoch [123/200], Loss: 0.0896\n",
      "Epoch [124/200], Loss: 0.0888\n",
      "Epoch [125/200], Loss: 0.0885\n",
      "Epoch [126/200], Loss: 0.0876\n",
      "Epoch [127/200], Loss: 0.0865\n",
      "Epoch [128/200], Loss: 0.0860\n",
      "Epoch [129/200], Loss: 0.0854\n",
      "Epoch [130/200], Loss: 0.0846\n",
      "Epoch [131/200], Loss: 0.0843\n",
      "Epoch [132/200], Loss: 0.0837\n",
      "Epoch [133/200], Loss: 0.0830\n",
      "Epoch [134/200], Loss: 0.0827\n",
      "Epoch [135/200], Loss: 0.0822\n",
      "Epoch [136/200], Loss: 0.0816\n",
      "Epoch [137/200], Loss: 0.0812\n",
      "Epoch [138/200], Loss: 0.0806\n",
      "Epoch [139/200], Loss: 0.0801\n",
      "Epoch [140/200], Loss: 0.0797\n",
      "Epoch [141/200], Loss: 0.0791\n",
      "Epoch [142/200], Loss: 0.0786\n",
      "Epoch [143/200], Loss: 0.0781\n",
      "Epoch [144/200], Loss: 0.0775\n",
      "Epoch [145/200], Loss: 0.0771\n",
      "Epoch [146/200], Loss: 0.0766\n",
      "Epoch [147/200], Loss: 0.0762\n",
      "Epoch [148/200], Loss: 0.0758\n",
      "Epoch [149/200], Loss: 0.0754\n",
      "Epoch [150/200], Loss: 0.0750\n",
      "Epoch [151/200], Loss: 0.0745\n",
      "Epoch [152/200], Loss: 0.0741\n",
      "Epoch [153/200], Loss: 0.0737\n",
      "Epoch [154/200], Loss: 0.0733\n",
      "Epoch [155/200], Loss: 0.0729\n",
      "Epoch [156/200], Loss: 0.0725\n",
      "Epoch [157/200], Loss: 0.0721\n",
      "Epoch [158/200], Loss: 0.0717\n",
      "Epoch [159/200], Loss: 0.0714\n",
      "Epoch [160/200], Loss: 0.0710\n",
      "Epoch [161/200], Loss: 0.0706\n",
      "Epoch [162/200], Loss: 0.0703\n",
      "Epoch [163/200], Loss: 0.0699\n",
      "Epoch [164/200], Loss: 0.0696\n",
      "Epoch [165/200], Loss: 0.0693\n",
      "Epoch [166/200], Loss: 0.0689\n",
      "Epoch [167/200], Loss: 0.0686\n",
      "Epoch [168/200], Loss: 0.0683\n",
      "Epoch [169/200], Loss: 0.0679\n",
      "Epoch [170/200], Loss: 0.0676\n",
      "Epoch [171/200], Loss: 0.0673\n",
      "Epoch [172/200], Loss: 0.0670\n",
      "Epoch [173/200], Loss: 0.0667\n",
      "Epoch [174/200], Loss: 0.0664\n",
      "Epoch [175/200], Loss: 0.0661\n",
      "Epoch [176/200], Loss: 0.0658\n",
      "Epoch [177/200], Loss: 0.0655\n",
      "Epoch [178/200], Loss: 0.0652\n",
      "Epoch [179/200], Loss: 0.0650\n",
      "Epoch [180/200], Loss: 0.0647\n",
      "Epoch [181/200], Loss: 0.0644\n",
      "Epoch [182/200], Loss: 0.0641\n",
      "Epoch [183/200], Loss: 0.0639\n",
      "Epoch [184/200], Loss: 0.0636\n",
      "Epoch [185/200], Loss: 0.0634\n",
      "Epoch [186/200], Loss: 0.0631\n",
      "Epoch [187/200], Loss: 0.0629\n",
      "Epoch [188/200], Loss: 0.0627\n",
      "Epoch [189/200], Loss: 0.0624\n",
      "Epoch [190/200], Loss: 0.0622\n",
      "Epoch [191/200], Loss: 0.0620\n",
      "Epoch [192/200], Loss: 0.0617\n",
      "Epoch [193/200], Loss: 0.0615\n",
      "Epoch [194/200], Loss: 0.0613\n",
      "Epoch [195/200], Loss: 0.0611\n",
      "Epoch [196/200], Loss: 0.0609\n",
      "Epoch [197/200], Loss: 0.0607\n",
      "Epoch [198/200], Loss: 0.0605\n",
      "Epoch [199/200], Loss: 0.0603\n",
      "Epoch [200/200], Loss: 0.0601\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize the hidden state\n",
    "    hidden = torch.zeros(1, batch_size, hidden_size)  # Shape: (num_layers, batch_size, hidden_size)\n",
    "    cell = torch.zeros(1, batch_size, hidden_size)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, (hidden, cell) = model(x_train, hidden, cell)\n",
    "    loss = criterion(outputs, y_train)  # Compute loss for all time steps\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample output (first sequence):\n",
      "Predicted: [0.0549446  0.5168408  1.073276   0.8731556  0.79805285 0.69636005\n",
      " 0.55725634 0.42824063 0.28370196 0.12557617]\n",
      "Target:    [1.         0.9870502  0.9485362  0.8854555  0.7994419  0.69272304\n",
      " 0.5680629  0.42869017 0.2782146  0.12053337]\n"
     ]
    }
   ],
   "source": [
    "# Test the model (inference)\n",
    "with torch.no_grad():\n",
    "    test_hidden = torch.zeros(1, batch_size, hidden_size)\n",
    "    test_cell = torch.zeros(1, batch_size, hidden_size)\n",
    "    test_outputs, _ = model(x_train, test_hidden, test_cell)\n",
    "    print(\"\\nSample output (first sequence):\")\n",
    "    print(\"Predicted:\", test_outputs[0].squeeze().numpy())\n",
    "    print(\"Target:   \", y_train[0].squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# device = torch.device('cpu')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cpu')\n",
    "x = torch.zeros(100, 2, dtype=torch.float32, device=device)\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
